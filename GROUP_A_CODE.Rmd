---
title: "Data Scientists Jobs in the United States"
authors: Dylan Rosenthal, Bengusu Ozcan, Silvia Sunseri, Davide Vaccari
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, warning = FALSE, message = FALSE)

library(dplyr)
library(ggplot2)
library(rgdal)
library(tmap)
library(readxl)
library(ggrepel)
library(sf)
library(DT)
library(htmltools)
library(leaflet)
library(quanteda)
library(magrittr)
#install.packages("cartography")
library(cartography)
library(tidytext)
library(plotrix)
library(devtools)
library(gdata)
library(RColorBrewer)
library(stringr)
library(wordcloud)
library(ggthemes)
library(ggmap)
library(textdata)
library(tidyverse)
require(scales)
library(wesanderson)
library(plotly)
library(tm)
#install.packages("qdapTools")
library(qdapTools)
#install.packages("textclean")
library(textclean)
df = read.csv("data_scientist.csv")
search = read.csv("GT_Data.csv")
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
key_words = read.csv("data scientist key words.csv")
```

The aim of our project is to investigate Data Science Jobs in the US. We are interested in the topic since we are studying to become Data Scientists, and therefore, would like to know more about it. We are going to use the data about Data Science Job Posting on Glassdoor. It was collected by web-scraping job posts from Glassdoor for data science jobs.

```{r message=FALSE, warning=FALSE}
df <- select(df, -(X))
```

```{r message=FALSE, warning=FALSE}
pal1 <- wes_palette(24, name = "Zissou1", type = "continuous")

df$Job.Title <- str_extract(df$Job.Title, "[^-]+") 
df$Job.Title <- str_extract(df$Job.Title, "[^/]+") 
df$Job.Title <- str_extract(df$Job.Title, "[^,]+") 
df <- filter(df, Job.Title != "E")
df <- filter(df, Job.Title != "AI")
df <- filter(df, Job.Title != "In")
df <- filter(df, Job.Title != "VP")
df <- filter(df, Job.Title != "Hydrogen")
df$Job.Title <- gsub("Senior", "", df$Job.Title)
df$Job.Title <- gsub("Sr.", "", df$Job.Title)
df$Job.Title <- gsub("Staff", "", df$Job.Title)
df$Job.Title[grepl('Analyst',df$Job.Title)] <- 'Data Analyst'
df$Job.Title[grepl('Scientist',df$Job.Title)] <- 'Data Scientist'
df$Job.Title[grepl('SCIENTIST',df$Job.Title)] <- 'Data Scientist'
df$Job.Title[grepl('Data Science',df$Job.Title)] <- 'Data Scientist'
df$Job.Title[grepl('Data Engineer',df$Job.Title)] <- 'Data Engineer'
df$Job.Title[grepl('ENGINEER',df$Job.Title)] <- 'Data Engineer'
df$Job.Title[grepl('Modeler',df$Job.Title)] <- 'Data Modeler'
df$Job.Title[grepl('Analytics',df$Job.Title)] <- 'Analytics'
df$Job.Title[grepl('Manager ',df$Job.Title)] <- 'Manager'
df$Job.Title[grepl('ML',df$Job.Title)] <- 'Machine Learning Engineer'

df %>%
select(Job.Title, max_salary) %>%
group_by(Job.Title) %>%
summarise(mean = mean(max_salary)) %>%
ggplot(aes(x = reorder(Job.Title, mean), y = mean, fill = Job.Title)) +
geom_col() +
coord_flip() +
labs(x = "Job Title", y = "Average Salary (in thousands of $)", caption = "Source: 2018 Kaggle Data Science Job Posting on Glassdor") +
ggtitle("Average salary by Job") +
scale_fill_manual(values=pal1) +
theme_minimal() +
theme(legend.title=element_blank(),
      legend.position = "none",
      plot.caption = element_text(hjust = -1.3, vjust = -2.3,size = 8, color ="azure4"),
      plot.title = element_text(hjust = -0.4, vjust= 2.5),
      axis.title.x = element_text(vjust = -0.5),
      axis.title.y = element_text(vjust = 3))
```

We have decided to start our analysis with a graph that could give a sort of general overview of Data Science Jobs (and related) and salaries. From the graph above, it can be seen that being a Manager is definitely the position that, on average, pays the most. Data Architect immediately follows with an average yearly salary of $200,000. Contrary to our expectation, Data Modeler does not earn that much and can be found at almost the end of the ranking.

### Data Science Salaries by Sector

In this part of the project, we are going to analyze which sectors have the highest salaries. We think this is a useful because it highlights the trend that, even in the field of data science, there can be significant differences in salary depending on the field that an individual may choose to go into.

```{r, eval=FALSE, message=FALSE, warning=FALSE}
# install.packages("wesanderson")
library(ggthemes)
library(plotly)
library(wesanderson)
library(ggrepel)
pal <- wes_palette(22, name = "FantasticFox1", type = "continuous")
```


```{r message=FALSE, warning=FALSE}
pal <- wes_palette(22, name = "FantasticFox1", type = "continuous")
d1 <- df %>% filter(!Sector=="-1") %>% group_by(Sector) %>% summarise(Job_Openings=n(), Average_Salary=mean(avg_salary)) %>% filter(Job_Openings >3) %>% ggplot() + 
geom_col(aes(x = reorder(Sector, Average_Salary), y = round(Average_Salary), fill=Sector, text=paste0("Average Salary:" ,round(Average_Salary)," 000s USD")), size = 1, color = "white") + 
  geom_line(aes(x = reorder(Sector,Average_Salary), y = Job_Openings), size = 0.5, color="gray35", group = 1)+
  geom_point(aes(x = reorder(Sector,Average_Salary), y = Job_Openings, text=paste("Number of Job Postings: ",Job_Openings)))+labs(x="Sectors", y="Average Annual Salary (in thousands of $)")+theme_tufte()+guides(color = guide_legend(override.aes = list(size = 0.1))) +theme(legend.position="bottom") +theme(legend.title = element_text(size = 5), legend.text = element_text(size = 5))+  
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
  theme(plot.title = element_text(size = 10, face = "bold", hjust = 0.5))+ 
theme(axis.text.x = element_blank())+
scale_fill_manual(values=pal)

#Below turns d1 visualization into interactive plotly graph
ggplotly(d1, tooltip="text") %>%
  layout(title = list(text = paste0('Data Related Job Posting Salaries per Sector',
                                    '<br>',
                                    '<sup>',
                                    'Covers job titles such as data scientist, data analyst, data engineer...',
                                    '<br>',
                                    '<sup>',
                                    'Hover over the black line to see number of job openings per sector.',
                                    '</sup>')), size=I(3),titlefont = list(size = 13))

```

The above graph shows the average salary for data-related job postings across different sectors. Highest paying sectors are Media and Retail. However, the total number of openings from those sectors in the data set are only 5 and 7 respectively. The sector with the highest opportunity for data related jobs seem to be Business Services and Information Technology, because, despite being on the average salary range across sectors, they have the highest number of openings and the biggest job opportunity.

### Interest in Data Science Jobs and Available Jobs per State

After exploring the relationship between sectors and salaries, we next try to assess the popularity of data science roles, and how it differentiates by state. We are assuming that Google search trends for data science related search terms (such as data modeler, data architect and data engineer) are an effective proxy for interest. We then compare how interest in data science roles, as measured by Google trends, is related to the actual number of job openings. For this purpose, we pulled Google Search trends for the following job titles across US states. Google trends provide a relative number of search popularity, not the absolute number of total searches.

```{r message=FALSE, warning=FALSE}
#Below code creates multiple data frames for subsetting the original data set into job titles

df$job_state <- trim(df$job_state)

search <- search %>% rename(job_state=State)

data_anly<- df %>% filter((grepl("Analyst",Job.Title))) %>%  group_by(job_state) %>% summarise(count=n())
d1 <- left_join(data_anly, search)

data_sci <- df %>% filter((grepl("Data Science",Job.Title)|grepl("Data Scientist",Job.Title))) %>%  group_by(job_state) %>% summarise(count=n())
d2 <- left_join(data_sci, search)

data_ml <- df %>% filter((grepl("Machine Learning",Job.Title))) %>%  group_by(job_state) %>% summarise(count=n())
d3 <- left_join(data_ml, search)

data_eng <- df %>% filter((grepl("Engineer",Job.Title))) %>%  group_by(job_state) %>% summarise(count=n())
d4 <- left_join(data_eng, search)


#Below code creates 4 graphs for each job title, showing total number of job openings vs Google Searches per state
v1<-d1 %>% ggplot() + geom_point(aes(x=count, y=Data.Analyst,text=paste("State: ",job_state))) + geom_smooth(aes(x=count, y=Data.Analyst,text="Data Analyst"),color="red", se=FALSE)+theme_minimal()+ theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank())

v2<-d2 %>% ggplot() + geom_point(aes(x=count, y=Data.Scientist,text=paste("State: ",job_state))) + geom_smooth(aes(x=count, y=Data.Scientist, text="Data Scientist"), color="blue", se=FALSE)+theme_minimal()+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

v3<-d3 %>% ggplot() + geom_point(aes(x=count, y=Machine.Learning.Engineer,text=paste("State: ",job_state))) + geom_smooth(aes(x=count, y=Machine.Learning.Engineer,text="Machine Learning Engineer"),color="orange", se=FALSE)+theme_minimal()+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

v4<-d4 %>% ggplot() + geom_point(aes(x=count, y=Data.Engineer,text=paste("State: ",job_state))) + geom_smooth(aes(x=count, y=Data.Engineer,text="Data Engineer"),color="purple", se=FALSE)+theme_minimal()+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

```

```{r message=FALSE, warning=FALSE}
#Below code generates interactive plotly plots and puts together all 4 charts

fig1<- ggplotly(v1,tooltip="text")
fig2<- ggplotly(v2,tooltip="text")
fig3<- ggplotly(v3,tooltip="text")
fig4<- ggplotly(v4,tooltip="text")

fig <- subplot(fig1,fig2,fig3,fig4, nrows =2) %>% layout(title = list(text = paste0('Google Searches vs Job Openings','<br>','<sup>',' X axis: Number of job postings    Y axis:Google Search trends, relative number indicating popularity, not exact number of searches')), size=I(2),titlefont = list(size = 13))

fig

```

We mostly see a strong correlation between the interest in job titles (as per Google search numbers) and the available jobs per state. California, without any surprise, is where the highest number of job openings and biggest interest is at. Interestingly, Virginia comes second in terms of openings and interest, rather than NY.

Another important insight is that there are many cases where the interest is not met by the available jobs. Texas, Illinois and Florida have very high Google search numbers for Data Science jobs but in terms of the number of job postings, they are in the smallest range.

The last bit of insight this graph shows is the interest for different job titles. We see that Data Analyst and Data Scientist searches on Google are quite high, on a scale of 0-50, while for Machine Learning Engineer and Data Engineer, this relative spectrum is quite smaller. 

### Geographical Breakdown of Data Science Job Openings and Salaries

```{r, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
install.packages('unikn')
library('unikn')
```

After analyzing the statewide breakdown of interest and availability of data science jobs, we thought it would be interesting to visualize on a map which cities and states have the most job openings and highest salaries.  

```{r message=FALSE, warning=FALSE}
ggmap::register_google(key = "AIzaSyB1-MjiXEIrgdT3FbflMLc8EUaQXVG3XVY")
```

#### Number of Openings

```{r message=FALSE, warning=FALSE}
states <- df %>%
  group_by(job_state) %>%
  count(job_state) %>%
  arrange(desc(n))

cities <- df %>%
  group_by(Location) %>%
  count(Location) %>%
  arrange(desc(n))
```

```{r}
# ggmap mapped some cities it couldn't find as "united states". Let's remove it.
cities <- cities[-10, ]
```

```{r message=FALSE, warning=FALSE}
datatable(states, filter = 'top', colnames = c("State", "Openings")) 
datatable(cities,filter = 'top', colnames = c("Cities", "Openings")) 
```

```{r}
cities <- cities %>%
  mutate(`Col_Bin` = case_when(
    n >= 7 & n < 22 ~ "second",
    n >= 22 ~ "first",
    TRUE ~ "other"
  ))
```

```{r message=FALSE, warning=FALSE, results='hide'}
GeoCoded <- purrr::map_df(.x = cities$Location, .f = ggmap::geocode)
```

```{r message=FALSE, warning=FALSE}
geocoded_df <- dplyr::bind_cols(cities, GeoCoded) %>% 
  dplyr::select(
    lng = lon,
    lat,
    dplyr::everything())
```

```{r message=FALSE, warning=FALSE, include=FALSE}
us <- readOGR(dsn = "cb_2018_us_state_500k", layer = "cb_2018_us_state_500k")
```

```{r message=FALSE, warning=FALSE}
states$popuptext <- base::paste0("<b>", 
                                 "Number of Openings: ",
                                 "</b><br />",
                                 states$n)
openings_by_state <- merge(us, states, by.x = "STUSPS", by.y = "job_state")
```

```{r message=FALSE, warning=FALSE}
geocoded_df$popuptext <- base::paste0("<b>", 
                                 "Number of Openings: ",
                                 "</b><br />",
                                 geocoded_df$n)
```

```{r, echo=FALSE,results='hide', include=FALSE}
bins <- c(0, 1, 3, 6, 10, 20, 30, 60, Inf)
blue <- seecol("pal_seeblau", 9)
blue[7] <- blue[8]
blue[8] <- blue[9]
blue[9] <- "royalblue4"
pal <- colorBin(blue, domain = states$n, bins = bins)
```

```{r}
pal2 <- c("royalblue4", "gray77", "lightskyblue1")
palpat = colorFactor(pal2, domain = geocoded_df$`Col_Bin`)
opening_color = palpat(geocoded_df$`Col_Bin`)
```

```{r message=FALSE, warning=FALSE}
map_openings <- leaflet(geocoded_df) %>%
  setView(lng = -98.5795, lat = 39.8283, zoom = 2.5) %>%
  addProviderTiles(providers$CartoDB.PositronNoLabels) %>%
  addPolygons(data = openings_by_state, 
            color = "white", 
            weight = 2, 
            smoothFactor = 0.5,
            opacity = 1.0, 
            fillOpacity = 0.9,
            dashArray = "2",
            fillColor = ~pal(n),
            highlightOptions = highlightOptions(color = "red", weight = 2,
                                                bringToFront = TRUE),
            popup = ~popuptext ,
            group = "States",
            label = ~htmlEscape(as.character(NAME))) %>%
  addCircleMarkers(color = ~palpat(`Col_Bin`),
             label = ~htmlEscape(as.character(Location)),
             fill = TRUE,
             lng = ~lng, 
             lat = ~lat,
             popup = ~popuptext,
             radius = ~n/3,
             group = "Cities") %>%
  addLayersControl(
     baseGroups = c("States", "Cities"),
     options = layersControlOptions(collapsed = FALSE)
  )

map_openings
```

**Note: grey states have no openings**

From the first map above we can see that the States with the more openings are California, Virginia, and Massachusetts. City-wide, the jobs are well distributed over the Country. However, the city with more openings are San Francisco with 69 and New York with 50. In the third place comes Washington D.C. with just 26 openings. Nonetheless, it has to be taken into consideration that the openings in the dataset are community specific. This means that there are a lot of openings in the surrounding areas of the big cities that are not, however, counted as if in their metro areas. Zooming in to San Francisco gives a clearer idea. About thirty-plus areas with openings surround the San Francisco's metropolitan area. Santa Clara has 9 openings, Redwood City has 7, San Jose 4, Cupertino 3, and many more.

#### Average Salary

```{r message=FALSE, warning=FALSE}
states_salary <- df %>%
  group_by(job_state) %>%
  dplyr::summarize(`Average Salary` = mean(avg_salary))
 
states_salary$`Average Salary` <-  round(states_salary$`Average Salary`, digits = 2)

cities_salary <- df %>%
  group_by(Location) %>%
  dplyr::summarize(`Average Salary` = mean(avg_salary))

cities_salary$`Average Salary` <-  round(cities_salary$`Average Salary`, digits = 2)

# Same as above, let's remove the generically labeled "united states".
cities_salary <- cities_salary[-186,]
```

```{r message=FALSE, warning=FALSE}
print("The median salary in the dataset is:")
median(cities_salary$`Average Salary`)
```

```{r message=FALSE, warning=FALSE}
cities_salary <- cities_salary %>%
  mutate(`Median` = ifelse(`Average Salary` >= 115, paste("Above National Median"), paste("Below National Median")))
```

```{r message=FALSE, warning=FALSE, results='hide'}
GeoCoded2 <- purrr::map_df(.x = cities_salary$Location, .f = ggmap::geocode)
```

```{r message=FALSE, warning=FALSE}
geocoded_df2 <- dplyr::bind_cols(cities_salary, GeoCoded2) %>% 
  dplyr::select(
    lng = lon,
    lat,
    dplyr::everything())
```

```{r message=FALSE, warning=FALSE}
states_salary$popuptext <- base::paste0("<b>", 
                                 "Average Salary: ",
                                 "</b><br />",
                                 states_salary$`Average Salary`)
salary_by_state <- merge(us, states_salary, by.x = "STUSPS", by.y = "job_state")
```

```{r message=FALSE, warning=FALSE}
geocoded_df2$popuptext <- base::paste0("<b>", 
                                 "City average salary: ",
                                 "</b><br />",
                                 geocoded_df2$`Average Salary`)
```

```{r message=FALSE, warning=FALSE}
bins2 <- c(90,100,110,120,130,140,150, Inf)
my_palette <- carto.pal("green.pal", 8)
pal2 <- colorBin(my_palette, domain = states_salary$`Average Salary`, bins = bins2)
```

```{r message=FALSE, warning=FALSE}
second_palette <- c("green3", "firebrick2")
pal3 = colorFactor(second_palette, domain = geocoded_df2$`Median`)
median_color = pal3(geocoded_df2$`Median`)
```


```{r message=FALSE, warning=FALSE}
map_salary <- leaflet(geocoded_df2) %>%
  setView(lng = -98.5795, lat = 39.8283, zoom = 2.5) %>%
  addProviderTiles(providers$CartoDB.PositronNoLabels) %>%
  addPolygons(data = salary_by_state, 
            color = "white", 
            weight = 2, 
            smoothFactor = 0.5,
            opacity = 1.0, 
            fillOpacity = 0.9,
            dashArray = "2",
            fillColor = ~pal2(`Average Salary`),
            highlightOptions = highlightOptions(color = "red", weight = 2,
                                                bringToFront = TRUE),
            popup = ~popuptext ,
            group = "States",
            label = ~htmlEscape(as.character(NAME))) %>%
  addCircleMarkers(color = ~pal3(`Median`),
             label = ~htmlEscape(as.character(Location)),
             fill = TRUE,
             lng = ~lng, 
             lat = ~lat,
             popup = ~popuptext,
             radius = ~`Average Salary`/18,
             group = "Cities") %>%
  addLegend(pal = pal3, values = ~geocoded_df2$Median, title = "Cities to National Median", group = "Cities") %>%
  addLayersControl(
     baseGroups = c("Cities", "States"),
     options = layersControlOptions(collapsed = FALSE)
  )

map_salary
```

**Note: grey states have no openings. Salaries are in thousands of dollar.**

Here, we wanted to focus more on the cities. The cities that have an average salary for their openings higher than the national median are colored in green. On the other hand, cities with average salaries below the national median are colored in red. Furthermore, the size of the circles is proportional to the city's average salary. Among the most known cities, and with a discrete number of openings, Dallas (TX) and Sacramento (CA) are the two with the highest average salary: `$`183,000. There are three cities with a salary of `$`271,000 and only one job opening. One in TX, one in CA and one in DE. Colorado Springs (CO) and Tulsa (OK) close the list with salaries of just 43k and 67k.

On the other hand, the states map shows the average salary per state. The state with the higher average is Delaware (with `$`271,000), but this is because it has just one opening. In the second place comes North Carolina with just slightly under `$`150,000. South Carolina and Montana close the list with, respectively, 95.5k and 93.75k.

### Text Analysis of Data Science Job Descriptions

Following our analysis of job openings and salaries, we wanted to analyze the descriptions of the job openings to better understand what skills and abilities are desired by potential employers. We achieve this goal by breaking down specific words that commonly show up in data science job descriptions, with a special focus on words that indicate a specific skill set of the applicants.

```{r message=FALSE, warning=FALSE}
# Add Unique ID for each row
df$id <- id(df)
```

```{r message=FALSE, warning=FALSE}
# Clean up data to prepare for text analysis
df$Job.Description <- replace_url(df$Job.Description,
                                 pattern = qdapRegex::grab("rm_url"),
                                 replacement = "")
df$Job.Description <- str_replace_all(df$Job.Description,
                                     pattern = "[[:digit:]]+",
                                     replacement = "")
df$Job.Description <- str_replace_all(df$Job.Description,
                                     pattern = "[[:punct:]]+",
                                     replacement = "")
df$Job.Description <- trim(df$Job.Description)
```

```{r message=FALSE, warning=FALSE}
df_unnest <- unnest_tokens(df, word, Job.Description)
```

```{r message=FALSE, warning=FALSE}
# remove stop words
df_unnest <- anti_join(df_unnest, stop_words, 
                       by = "word")
```

```{r message=FALSE, warning=FALSE}
# only keep useful words
df_unnest <- inner_join(df_unnest, key_words, 
                        by = "word")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
(word_count <- df_unnest %>% group_by(word, word_type) %>% 
  summarise(count = n(), avg_salary = round((mean(avg_salary)), digits = 1))) %>%
  arrange(desc(count))
```

```{r message=FALSE, warning=FALSE}
# Create a wordcloud of word count for skills
skills_word_count <- filter(word_count, word_type == "skill")
wordcloud(skills_word_count$word, skills_word_count$count,
          max.words = 100, colors = "pink3")
```

The word cloud above shows the most common skills mentioned in data science job descriptions. Some of the most common are obvious, like machine learning, modeling, knowing data languages and how to use certain applications. Some of the most common are a little more surprising, like having strong writing and communication skills, or being a good collaborator.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Create term document matrix to compare most common skills and tools in data scientist versus data analyst job postings
analyst_sci_tdm <- df_unnest %>% filter(word_type != "task",
                                        Job.Title == "Data Scientist" |
                                        Job.Title == "Data Analyst") %>% group_by(Job.Title, word) %>% summarise(count = n()) %>% cast_tdm(word, Job.Title, count)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
analyst_sci_matrix <- as.matrix(analyst_sci_tdm)
```

```{r message=FALSE, warning=FALSE}
# Create comparison cloud for skills and tools in data science vs. data analyst jobs
comparison.cloud(analyst_sci_matrix, colors = c("salmon", "darkslategray4"), title.size= 2, 
                 max.words = 100, title.bg.colors = "peachpuff2")
```

The comparison cloud above shows the most common skills and tools mentioned in data scientist versus data analyst job descriptions. The words in the data scientist job descriptions appear to be much more technical, such as machine learning, engineering, and modeling, while words in data analyst job descriptions are much more focused around the use of specific tools used for data analysis (such as Microsoft and Tableau) and communicating insights through dashboards and presentations.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Create term document matrix to compare most common skills and tools in data scientist versus data analyst job postings
tool_tdm <- df_unnest %>% filter(word_type == "tool",
                                        Job.Title == "Data Scientist" |
                                        Job.Title == "Data Analyst") %>% group_by(Job.Title, word) %>% summarise(count = n()) %>% cast_tdm(word, Job.Title, count)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
tool_matrix <- as.matrix(tool_tdm)
```

```{r message=FALSE, warning=FALSE}
# Create comparison cloud for tools in data science vs. data analyst jobs
comparison.cloud(tool_matrix, colors = c("salmon", "darkslategray4"), title.size= 2, 
                 max.words = 100, title.bg.colors = "peachpuff2")
```

The comparison cloud above shows the most common tools mentioned in data scientist versus data analyst job descriptions. Once again, we can see that data scientist roles appear to be much more technical and oriented around coding for data analysis, with tools such as Python, SAS and Java topping the list, while data analysis tools (with the exception of SQL) are more focused around general business intelligence tools used for less advanced analysis, such as Excel, Tableau, and Microsoft.


### Gender Pay Gap in Data Science Jobs

In this part of the project, we would like to focus on the gender pay gap among Data Scientist in the US. Unfortunately, it has been [studied](https://towardsdatascience.com/gender-pay-gap-among-data-scientists-on-kaggle-87b393aa21fe) that male data scientists in the US earn more and have a higher earnings range than their female counterparts. Furthermore, it has been [demonstrated](https://www.nature.com/articles/d41586-019-00611-1) that more than 40% of women with full-time jobs in science leave the sector or go part time after having their first child in the United States. By contrast, only 23% of new fathers leave or cut their working hours. We would like to see if these findings are also supported by the data that we have. We are going to use the 2018 Kaggle Machine Learning & Data Science Survey, the most comprehensive dataset available on the state of ML and data science.

```{r message=FALSE, warning=FALSE}
survey <- read.csv("Gender gap in Data Science/multipleChoiceResponses.csv")
```

```{r message=FALSE, warning=FALSE}
survey <- survey %>% filter(Q3 == "United States of America" | Q3 == "In which country do you currently reside?")
```

```{r message=FALSE, warning=FALSE}
survey_salary <- survey %>%
  filter(Q9 != "I do not wish to disclose my approximate yearly compensation") 
```

```{r message=FALSE, warning=FALSE}
survey_salary$min <- str_extract(survey_salary$Q9, "[^-]+")
survey_salary$max <- gsub( "(.*)-(.*)", "\\2",  survey_salary$Q9)
survey_salary$max1 <- str_extract(survey_salary$max, "[^,]+")
```

```{r message=FALSE, warning=FALSE}
survey_salary$min <- as.numeric(survey_salary$min)
survey_salary$max1 <- as.numeric(survey_salary$max1)
survey_salary <- survey_salary %>% mutate(average_salary = (((min + max1)/2)*1000))
```

```{r message=FALSE, warning=FALSE}
survey_salary$Q1[survey_salary$Q1=="Prefer not to say"] <- "Other"
survey_salary$Q1[survey_salary$Q1=="Prefer to self-describe"] <- "Other"
survey_salary <- survey_salary%>%
  filter(Q3 != "In which country do you currently reside?")
```

```{r message=FALSE, warning=FALSE}
GeomSplitViolin <- ggproto(
  "GeomSplitViolin", 
  GeomViolin, 
  draw_group = function(self, data, ..., draw_quantiles = NULL) {
    data <- transform(data, 
                      xminv = x - violinwidth * (x - xmin), 
                      xmaxv = x + violinwidth * (xmax - x))
    grp <- data[1,'group']
    newdata <- plyr::arrange(
      transform(data, x = if(grp%%2==1) xminv else xmaxv), 
      if(grp%%2==1) y else -y
    )
    newdata <- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])
    newdata[c(1,nrow(newdata)-1,nrow(newdata)), 'x'] <- round(newdata[1, 'x']) 
    if (length(draw_quantiles) > 0 & !scales::zero_range(range(data$y))) {
      stopifnot(all(draw_quantiles >= 0), all(draw_quantiles <= 1))
      quantiles <- ggplot2:::create_quantile_segment_frame(data, draw_quantiles)
      aesthetics <- data[rep(1, nrow(quantiles)), setdiff(names(data), c("x", "y")), drop = FALSE]
      aesthetics$alpha <- rep(1, nrow(quantiles))
      both <- cbind(quantiles, aesthetics)
      quantile_grob <- GeomPath$draw_panel(both, ...)
      ggplot2:::ggname("geom_split_violin", 
                       grid::grobTree(GeomPolygon$draw_panel(newdata, ...), quantile_grob))
    } else {
      ggplot2:::ggname("geom_split_violin", GeomPolygon$draw_panel(newdata, ...))
    }
  }
)

geom_split_violin <- function (mapping = NULL, 
                               data = NULL, 
                               stat = "ydensity", 
                               position = "identity", ..., 
                               draw_quantiles = NULL, 
                               trim = TRUE, 
                               scale = "area", 
                               na.rm = FALSE, 
                               show.legend = NA, 
                               inherit.aes = TRUE) {
  layer(data = data, 
        mapping = mapping, 
        stat = stat, 
        geom = GeomSplitViolin, 
        position = position, 
        show.legend = show.legend, 
        inherit.aes = inherit.aes, 
        params = list(trim = trim, 
                      scale = scale, 
                      draw_quantiles = draw_quantiles, 
                      na.rm = na.rm, ...)
        )
}

survey_salary %>%
  filter(Q1 != "Other") %>%
  filter(Q4 %in% c("Doctoral degree", "Masterâ€™s degree"))  %>%
  ggplot(aes(x = Q1, y = average_salary, fill = Q4)) +
  geom_split_violin(trim=FALSE) +
  theme_minimal() +
  scale_y_continuous(labels = comma) +
  scale_fill_manual(values = wes_palette(2, name = "FantasticFox1")) +
  labs(x = "Gender", y = "Average yearly salary", title = "Average Data Scientists' Salary by Educational Level and Gender", caption = "Source: 2018 Kaggle Machine Learning & Data Science Survey", fill = "Educational level") +
  theme(plot.caption = element_text(hjust = -0.4, vjust = -2,size = 8, color ="azure4"),
        axis.title.y = element_text(size = 11, vjust = 3),
        axis.title.x=element_text(vjust = -0.7))
```

The above graph shows the average Data Scientists' salary by educational level and gender. First of all, it can be seen that, having a Doctoral Degree instead of a Master's Degree, allows people to earn more, independently of their sex. However, it can be seen that women with both a Doctoral and a Master's Degree earn less than men. Also, the earnings for men are more spread out and can reach $500,000 per year, while this does not happen for women. From this graph, it can be concluded that a gender pay gap among Data Scientist in the US actually exists.


```{r message=FALSE, warning=FALSE}
survey_salary$Q2[survey_salary$Q2=="18-21"] <- "18-24"
survey_salary$Q2[survey_salary$Q2=="22-24"] <- "18-24"
survey_salary$Q2[survey_salary$Q2=="70-79"] <- "70+"
survey_salary$Q2[survey_salary$Q2=="80+"] <- "70+"
```

```{r message=FALSE, warning=FALSE}
#TAKE OTHER OUT
#ALSO NOT MANY SENIOR DATA SCIENTISTS
#CHANGE TO GEOM_BAR

survey_salary %>%
  filter(Q1 != "Other") %>%
  ggplot(aes(x = Q2, y = average_salary)) +
  geom_bar(aes(fill = Q1), stat="identity", position = position_dodge()) +
  scale_y_continuous(labels = comma) +
  theme_minimal() +
  labs(x = "Age", y = "Average yearly salary", title = "Average Data Scientists' Salary by Age and Gender", caption = "Source: 2018 Kaggle Machine Learning & Data Science Survey", fill = "Gender") +
  scale_fill_manual(values = c("rosybrown1", "cadetblue2")) +
  theme(plot.caption = element_text(hjust = -0.3, vjust = -2.3,size = 8, color ="azure4"),
        axis.title.y = element_text(size = 11, vjust = 3),
        axis.title.x=element_text(vjust = -0.7))
```

The above graph shows the average Data Scientists' salary by age and gender. It can be seen that, at the beginning of their working career, men and women earn almost the same. However, when the age increases, at about 30 years old, the difference between the salary for men vs the one for women is evident. Thirty years is the age when a woman usually becomes a mother and, precisely in conjunction with this age group, the differences between the male and female wages are clear. Men earn almost twice as much as women. As the article by Nature suggests, "parenthood is an important driver of gender imbalance in STEM employment." In conclusion, we hope that the gap will close in the future.

In the same repository, you can find a [process book](https://github.com/QMSS-G5063-2021/GroupA_Data_Science_Jobs/blob/main/PROCESS%20BOOK.pdf) with a description of the steps taken to clean the data and create our graphs.


```{r}
```


















